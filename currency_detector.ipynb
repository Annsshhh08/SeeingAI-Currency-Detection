{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "root_train_dir = \"/home/xiaoyzhu/notebooks/currency_detector/data/train\"\n",
    "root_test_dir = \"/home/xiaoyzhu/notebooks/currency_detector/data/test\"\n",
    "root_validation_dir = \"/home/xiaoyzhu/notebooks/currency_detector/data/validation\"\n",
    "root_visualizaion_dir = \"/home/xiaoyzhu/notebooks/currency_detector/data/visualization\"\n",
    "saved_model_file_name = \"currency_detector_mobilenet.h5\"\n",
    "tensorboard_dir = \"/home/xiaoyzhu/notebooks/currency_detector/data/tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2092 images belonging to 14 classes.\n",
      "Found 420 images belonging to 14 classes.\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 0.9529 - acc: 0.6958 - val_loss: 2.7200 - val_acc: 0.3922\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.72004, saving model to currency_detector_test.h5\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 22s 220ms/step - loss: 0.2925 - acc: 0.9098 - val_loss: 2.1659 - val_acc: 0.5637\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.72004 to 2.16590, saving model to currency_detector_test.h5\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 0.2058 - acc: 0.9326 - val_loss: 2.3575 - val_acc: 0.4623\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 22s 218ms/step - loss: 0.1653 - acc: 0.9453 - val_loss: 2.5877 - val_acc: 0.4641\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 23s 233ms/step - loss: 0.1403 - acc: 0.9509 - val_loss: 4.3082 - val_acc: 0.3134\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 21s 214ms/step - loss: 0.1210 - acc: 0.9606 - val_loss: 1.4979 - val_acc: 0.6144\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.16590 to 1.49794, saving model to currency_detector_test.h5\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 22s 216ms/step - loss: 0.1121 - acc: 0.9667 - val_loss: 1.2741 - val_acc: 0.7055\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.49794 to 1.27406, saving model to currency_detector_test.h5\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 0.1332 - acc: 0.9524 - val_loss: 1.4177 - val_acc: 0.6797\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 21s 209ms/step - loss: 0.0951 - acc: 0.9684 - val_loss: 5.7170 - val_acc: 0.2386\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 23s 229ms/step - loss: 0.1055 - acc: 0.9623 - val_loss: 3.0890 - val_acc: 0.4743\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 20s 203ms/step - loss: 0.0934 - acc: 0.9675 - val_loss: 6.3876 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 22s 224ms/step - loss: 0.0795 - acc: 0.9776 - val_loss: 0.9632 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.27406 to 0.96319, saving model to currency_detector_test.h5\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 20s 202ms/step - loss: 0.0816 - acc: 0.9747 - val_loss: 2.5021 - val_acc: 0.5245\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 21s 207ms/step - loss: 0.0769 - acc: 0.9732 - val_loss: 1.9011 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 22s 216ms/step - loss: 0.0604 - acc: 0.9781 - val_loss: 1.7400 - val_acc: 0.6846\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 21s 206ms/step - loss: 0.0754 - acc: 0.9752 - val_loss: 2.2374 - val_acc: 0.5621\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 22s 224ms/step - loss: 0.0778 - acc: 0.9745 - val_loss: 3.1849 - val_acc: 0.4880\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 20s 204ms/step - loss: 0.0709 - acc: 0.9770 - val_loss: 1.6451 - val_acc: 0.6912\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 22s 220ms/step - loss: 0.0748 - acc: 0.9783 - val_loss: 2.5456 - val_acc: 0.5771\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 21s 208ms/step - loss: 0.0576 - acc: 0.9803 - val_loss: 1.7156 - val_acc: 0.6585\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 20s 201ms/step - loss: 0.0628 - acc: 0.9803 - val_loss: 2.2384 - val_acc: 0.6130\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 21s 215ms/step - loss: 0.0545 - acc: 0.9831 - val_loss: 3.2690 - val_acc: 0.5065\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5af4402828>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "img_width, img_height = 224,224\n",
    "train_data_dir = root_train_dir\n",
    "validation_data_dir = root_validation_dir\n",
    "nb_train_samples = 1672\n",
    "nb_validation_samples = 560\n",
    "train_steps = 100 # 1672 training samples/batch size of 32 = 52 steps. We are doing heavy data processing so put 500 here\n",
    "validation_steps = 20 # 560 validation samples/batch size of 32 = 10 steps. We put 20 for validation steps\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "def build_model():\n",
    "    # constructing the model\n",
    "    model = applications.mobilenet.MobileNet(weights=\"imagenet\", include_top=False, input_shape=(img_width, img_height, 3),\n",
    "                                  pooling='avg')\n",
    "\n",
    "    # only train the last 2 layers\n",
    "    for layer in model.layers[:-10]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Adding custom Layers\n",
    "    x = model.output\n",
    "    # x = Flatten()(x)\n",
    "    predictions = Dense(14, activation=\"softmax\")(x)\n",
    "\n",
    "    # creating the final model\n",
    "    model_final = Model(inputs=model.input, outputs=predictions)\n",
    "    \n",
    "    return model_final\n",
    "\n",
    "model_final = build_model()\n",
    "# compile the model\n",
    "model_final.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "# Initiate the train and test generators with data Augumentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    fill_mode=\"nearest\",\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    rotation_range=180)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    fill_mode=\"nearest\",\n",
    "    zoom_range=0.3,\n",
    "    rotation_range=30)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    # save_to_dir = root_visualizaion_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "# Save the model according to the conditions\n",
    "checkpoint = ModelCheckpoint(\"currency_detector_test.h5\", monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "# Train the model\n",
    "model_final.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps = validation_steps,\n",
    "    workers=16,\n",
    "    callbacks=[checkpoint, early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "img_width, img_height = 224,224\n",
    "image_scale = 1./255 # or 1./255 if you want to rescale (which should be the case)\n",
    "\n",
    "nb_train_samples = 1672\n",
    "nb_validation_samples = 560\n",
    "train_steps = 100 # 1672 training samples/batch size of 32 = 52 steps. We are doing heavy data processing so put 500 here\n",
    "validation_steps = 20 # 560 validation samples/batch size of 32 = 10 steps. We put 20 for validation steps\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "def build_model():\n",
    "    # constructing the model\n",
    "    model = applications.mobilenet.MobileNet(weights=\"imagenet\", include_top=False, \n",
    "                                             input_shape=(img_width, img_height, 3), pooling='avg')\n",
    "    # only train the last 2 layers\n",
    "    for layer in model.layers[:-10]:\n",
    "        layer.trainable = False\n",
    "    # Adding custom Layers\n",
    "    x = model.output\n",
    "    predictions = Dense(14, activation=\"softmax\")(x)\n",
    "    # creating the final model\n",
    "    model_final = Model(inputs=model.input, outputs=predictions)\n",
    "    \n",
    "    return model_final\n",
    "\n",
    "model_final = build_model()\n",
    "# compile the model\n",
    "model_final.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "# Initiate the train and test generators with data Augumentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=image_scale,\n",
    "    fill_mode=\"nearest\",\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    rotation_range=360)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=image_scale,\n",
    "    fill_mode=\"nearest\",\n",
    "    zoom_range=0.3,\n",
    "    rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=image_scale,\n",
    "    fill_mode=\"nearest\",\n",
    "    zoom_range=0.3,\n",
    "    rotation_range=30)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    root_train_dir,\n",
    "    # save_to_dir = root_visualizaion_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    root_validation_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    root_test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "# Class index\n",
    "print(\"training labels are:\", validation_generator.class_indices)\n",
    "\n",
    "# Save the model according to the conditions\n",
    "checkpoint = ModelCheckpoint(saved_model_file_name, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             mode='auto', period=1)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir = tensorboard_dir)\n",
    "# Train the model\n",
    "history = model_final.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps = validation_steps,\n",
    "    workers=16,\n",
    "    callbacks=[checkpoint, earlystopping, tensorboard])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model and generate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probabilities = model_final.predict_generator(test_generator, workers = 16, verbose = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(test_generator.class_indices)\n",
    "y_true = ((test_generator.classes))\n",
    "y_pred = (np.argmax(probabilities, axis = 1))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "name = ['5_dollar_back', '2_dollar_back', '1_dollar_back', '2_dollar_front', '20_dollar_front', '10_dollar_back', '50_dollar_back', '100_dollar_front', '1_dollar_front', '50_dollar_front', '20_dollar_back', '5_dollar_front', '10_dollar_front', '100_dollar_back']\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(confusion_matrix, classes=name,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# # Plot normalized confusion matrix\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(confusion_matrix, classes=name, normalize=True,\n",
    "#                       title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(test_generator)\n",
    "y_true = validation_generator.classes \n",
    "\n",
    "\n",
    "y_true = np.array([0] * 1000 + [1] * 1000)\n",
    "y_pred = probabilities > 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert models to ONNX for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxmltools\n",
    "import coremltools\n",
    "\n",
    "# install from https://github.com/onnx/onnxmltools and https://github.com/apple/coremltools\n",
    "\n",
    "model_coreml = coremltools.converters.keras.convert(saved_model_file_name, image_scale = image_scale)\n",
    "model_onnx = onnxmltools.convert.convert_coreml(model_coreml, \"currency_detector\")\n",
    "\n",
    "# Save as protobuf\n",
    "onnxmltools.utils.save_model(model_onnx, saved_model_file_name + \".onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils to Move data around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility to move data around\n",
    "def move_files_subfolders(root_src_dir, root_target_dir, operation, image_number):\n",
    "    for src_dir, dirs, files in os.walk(root_src_dir):\n",
    "        num_temp = 0\n",
    "        dst_dir = src_dir.replace(root_src_dir, root_target_dir)\n",
    "        if not os.path.exists(dst_dir):\n",
    "            os.mkdir(dst_dir)\n",
    "        for individual_file in files:\n",
    "            if num_temp < image_number:\n",
    "                src_file = os.path.join(src_dir, individual_file)\n",
    "                dst_file = os.path.join(dst_dir, individual_file)\n",
    "                if os.path.exists(dst_file):\n",
    "                    os.remove(dst_file)\n",
    "                if operation is 'copy':\n",
    "                    shutil.copy(src_file, dst_dir)\n",
    "                elif operation is 'move':\n",
    "                    shutil.move(src_file, dst_dir)\n",
    "                num_temp += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "# move_files_subfolders(root_train_dir, root_validation_dir,  'move', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
